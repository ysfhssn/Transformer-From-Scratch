# Transformer From Scratch

## Overview
<p align="center">
    <img src="https://i.stack.imgur.com/eAKQu.png" height="300"/>&nbsp;
    <img src="https://tungmphung.com/wp-content/uploads/2021/01/Screenshot-from-2021-01-10-19-27-31.png" height="300"/>
</p>

## Positional encoding
<p align="center">
    <img src="https://i.stack.imgur.com/PejTL.png" height="200"/>
    <img src="https://i.stack.imgur.com/PxeeE.png" height="100"/>
</p>

## Multi-Head Attention
<p align="center"><img src="https://coriva.eu.org/images/multiheadattention.png" height="300"/></p>

> ### Masked Attention (opt.)
<p align="center"><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1638824585791/vkXCmdGyw.png" height="200"/></p>

## References
[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
Attention Is All You Need.
*arXiv preprint* [*arXiv:1706.03762*](https://arxiv.org/pdf/1706.03762v5.pdf), 2017.
[2] [Tensorflow Transformer Tutorial](https://www.tensorflow.org/text/tutorials/transformer)
